### References
[1] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.

[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[3] Ivano Lauriola, Alberto Lavelli, and Fabio Aiolli. An introduction to deep learning in natural
language processing: Models, techniques, and tools. Neurocomputing, 470:443–456, 2022.
[4] Ahmet Iscen, Thomas Bird, Mathilde Caron, Alireza Fathi, and Cordelia Schmid. A memory
transformer network for incremental learning. arXiv preprint arXiv:2210.04485, 2022.

[5] Rahul Manohar Samant, Mrinal Bachute, Shilpa Gite, and Ketan Kotecha. Framework for deep
learning-based language models using multi-task learning in natural language understanding: A
systematic literature review and future directions. IEEE Access, 2022.

[6] Chulun Zhou, Zhihao Wang, Shaojie He, Haiying Zhang, and Jinsong Su. A novel multi-domain
machine reading comprehension model with domain interference mitigation. Neurocomputing,
500:791–798, 2022.

[7] Young Jin Kim and Hany Hassan Awadalla. Fastformers: Highly efficient transformer models
for natural language understanding. arXiv preprint arXiv:2010.13382, 2020.

[8] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Real-time inference
in multi-sentence tasks with deep pretrained transformers. arXiv preprint arXiv:1905.01969,
2019.

[9] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019.

[10] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv
preprint arXiv:1909.11942, 2019.

[11] Hyunjin Choi, Judong Kim, Seongho Joe, and Youngjune Gwon. Evaluation of bert and
albert sentence embedding performance on downstream nlp tasks. In 2020 25th International
conference on pattern recognition (ICPR), pages 5482–5487. IEEE, 2021.

[12] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for
word representation. In Proceedings of the 2014 conference on empirical methods in natural
language processing (EMNLP), pages 1532–1543, 2014.

[13] Akshay Khatri et al. Sarcasm detection in tweets with bert and glove embeddings. arXiv
preprint arXiv:2006.11512, 2020.

[14] Leilei Gan, Zhiyang Teng, Yue Zhang, Linchao Zhu, Fei Wu, and Yi Yang. Semglove: Semantic
co-occurrences for glove from bert. IEEE/ACM Transactions on Audio, Speech, and Language
Processing, 30:2696–2704, 2022.

[15] Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney. What happens to bert
embeddings during fine-tuning? arXiv preprint arXiv:2004.14448, 2020.

[16] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in
neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence,
44(9):5149–5169, 2021.

[17] Jane X Wang. Meta-learning in natural and artificial intelligence. Current Opinion in Behavioral
Sciences, 38:90–95, 2021.

[18] Simon Graham, Quoc Dang Vu, Mostafa Jahanifar, Shan E Ahmed Raza, Fayyaz Minhas, David
Snead, and Nasir Rajpoot. One model is all you need: multi-task learning enables simultaneous
histology image segmentation and classification. Medical Image Analysis, 83:102685, 2023.

[19] Michael Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint
arXiv:2009.09796, 2020.

[20] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances
in neural information processing systems, 31, 2018.

[21] Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep
neural networks with multitask learning. In Proceedings of the 25th international conference
on Machine learning, pages 160–167, 2008.

[22] Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently
identifying task groupings for multi-task learning. Advances in Neural Information Processing
Systems, 34:27503–27516, 2021.

[23] Shijie Chen, Yu Zhang, and Qiang Yang. Multi-task learning in natural language processing:
An overview. arXiv preprint arXiv:2109.09138, 2021.

[24] Yu Zhang and Qiang Yang. An overview of multi-task learning. National Science Review,
5(1):30–43, 2018.

[25] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International
conference on machine learning, pages 794–803. PMLR, 2018.

[26] Xian Li and Hongyu Gong. Robust optimization for multilingual translation with imbalanced
data. Advances in Neural Information Processing Systems, 34:25086–25099, 2021.

[27] Liyang Liu, Yi Li, Zhanghui Kuang, J Xue, Yimin Chen, Wenming Yang, Qingmin Liao, and
Wayne Zhang. Towards impartial multi-task learning. iclr, 2021.

[28] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems,
33:5824–5836, 2020.

[29] Lucas Mansilla, Rodrigo Echeveste, Diego H Milone, and Enzo Ferrante. Domain generalization
via gradient surgery. In Proceedings of the IEEE/CVF International Conference on Computer
Vision, pages 6630–6638, 2021.

[30] Xiaojun Zhou, Yuan Gao, Chaojie Li, and Zhaoke Huang. A multiple gradient descent design
for multi-task learning on edge computing: Multi-objective machine learning approach. IEEE
Transactions on Network Science and Engineering, 9(1):121–133, 2021.

[31] Qiwei Bi, Jian Li, Lifeng Shang, Xin Jiang, Qun Liu, and Hanfang Yang. Mtrec: Multi-task
learning over bert for news recommendation. In Findings of the Association for Computational
Linguistics: ACL 2022, pages 2663–2669, 2022.

[32] Tao Qi, Fangzhao Wu, Chuhan Wu, Peiru Yang, Yang Yu, Xing Xie, and Yongfeng Huang.
Hierec: Hierarchical user interest modeling for personalized news recommendation. arXiv
preprint arXiv:2106.04408, 2021.

[33] Jaekeol Choi, Euna Jung, Jangwon Suh, and Wonjong Rhee. Improving bi-encoder document
ranking models with two rankers and multi-teacher distillation. In Proceedings of the 44th
International ACM SIGIR Conference on Research and Development in Information Retrieval,
pages 2192–2196, 2021.

[34] Derrick Xin, Behrooz Ghorbani, Ankush Garg, Orhan Firat, and Justin Gilmer. Do current
multi-task optimization methods in deep learning even help? arXiv preprint arXiv:2209.11379,
2022.

[35] Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.
Which tasks should be learned together in multi-task learning? In International Conference on
Machine Learning, pages 9120–9132. PMLR, 2020.

[36] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
Processing. Association for Computational Linguistics, 11 2019.

[37] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. Advances in neural information
processing systems, 26, 2013.

[38] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus.
Hard negative mixing for contrastive learning. Advances in Neural Information Processing
Systems, 33:21798–21809, 2020.

[39] Xiao Wang, Yuhang Huang, Dan Zeng, and Guo-Jun Qi. Caco: Both positive and negative
samples are directly learnable via cooperative-adversarial contrastive learning. arXiv preprint
arXiv:2203.14370, 2022.

[40] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint arXiv:1902.10197, 2019.
10

[41] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y
Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a
sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural
language processing, pages 1631–1642, 2013.

[42] Travis Addair. Duplicate question pair detection with deep learning. Stanf. Univ. J, 2017.

[43] Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo. * sem 2013
shared task: Semantic textual similarity. In Second joint conference on lexical and computational
semantics (* SEM), volume 1: proceedings of the Main conference and the shared task: semantic
textual similarity, pages 32–43, 2013.

[44] Yile Wang, Leyang Cui, and Yue Zhang. How can bert help lexical semantics tasks? arXiv
preprint arXiv:1911.02929, 2019.

[45] D Viji and S Revathy. A hybrid approach of weighted fine-tuned bert extraction with deep
siamese bi–lstm model for semantic text similarity identification. Multimedia Tools and Applications, 81(5):6131–6157, 2022.

[46] Tianxin Wang, Fuzhen Zhuang, Ying Sun, Xiangliang Zhang, Leyu Lin, Feng Xia, Lei He, and
Qing He. Adaptively sharing multi-levels of distributed representations in multi-task learning.
Information Sciences, 591:226–234, 2022.

[47] Dripta S Raychaudhuri, Yumin Suh, Samuel Schulter, Xiang Yu, Masoud Faraki, Amit K
Roy-Chowdhury, and Manmohan Chandraker. Controllable dynamic multi-task architectures.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 10955–10964, 2022.

[48] Akari Asai, Mohammadreza Salehi, Matthew E Peters, and Hannaneh Hajishirzi. Attempt:
Parameter-efficient multi-task tuning via attentional mixtures of soft prompts. In Proceedings of
the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6655–6672,
2022.
